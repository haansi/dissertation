\chapter{Detecting Contamination in Next-Generation Sequencing Studies}
\label{chapterContamination}
With the new technological progress that comes with the DNA sequencing based on massive parallel sequencing, there arise also some drawbacks. The shorter read-length compared to Sanger-based Sequencing can be targeted by the so called Third Generation Sequencing. Here companies like Oxford Nanopore Technologies\footnote{\url{https://nanoporetech.com}}, Pacific Biosciences\footnote{\url{http://www.pacb.com/}} or BioNano Genomics\footnote{\url{http://bionanogenomics.com/}} already conquer the market for sequencing devices. Another issue, that becomes more and more prevalent, is the immanent problem of contamination, which becomes apparent based on the very sensitive NGS technology. While Sanger based Sequencing is able to detect heteroplasmic variants down to the 10\% level for the minor allele \cite{Kloss-Brandstatter2015}, NGS can detect heteroplasmic variants down to the 1\% minor allele level and even lower \cite{Li2010}. Also the expanding amount of sequencing studies increases the probability of errors to occur. The error can come in many forms in the lab, but also in the data processing steps. While in the lab or while sequencing, sample contamination, barcode contamination, cross-species contamination, carry-over contamination from previous run, issues with PCR products, issues with fragmentation of DNA, wrong protocols and many more can become problematic. 

The post-processing from the raw sequencing reads to the annotated variants can also lead to errors, due to wrong methods applied, with the wrong parameters, wrong reference sequences or systematic errors as previously described (see Section \ref{hg:phantom}). Issues with the wrong reference sequence is especially the case for mitochondrial sequencing studies where different reference sequences are in use \cite{Behar2012,Andrews1999}). One option to avoid post-processing errors, is the use of automated pipelines, in order to reduce the risk of such issues. 

Quality control is a second important option. Therefore in this chapter the concepts described in Chapter \ref{chapterHaplogrep} and Chapter \ref{chap:NGS} get combined to check mitochondrial data derived from massive parallel sequencing for the presence of contamination. Here, the concept is to use low-level heteroplasmy to help identifying an extra haplogroup in the sample of interest (see section \ref{haplochecker}. The concept was drafted in Li et al. \cite{Li2010} based on manual inspection with Phylotree. Avital et al. \cite{Avital2012} took advantage of HaploGrep in their pipeline, which was however not made available. A recent discussion highlights the problem of contamination in mitochondrial NGS studies \cite{Ye2014,Just2014, Just2015,Ye2014reply} and tools for detection of contamination are emerging \cite{Renaud2015,Jun2012,Dickins2014}, as presented in the Section \ref{cont:relatedwork}. The performance of the new workflow called HaploChecker is presented in the Section \ref{cont:result}, and the method is abstracted in the last Section \ref{cont:outlook}, by highlighting future applications as well as opportunities for improvements.

\section{Related Work}\label{cont:relatedwork}
The need for a quality check of sequencing data has been addressed in the past for Sanger based data \cite{Walker2004, Montesino2007, Bandelt2009, Yao2007} as well as for NGS based data \cite{Holland2011}. 
Concepts for detecting and correcting contamination were presented for nuclear DNA NGS sequencing projects, where MicroArray data are available, in Jun et al. \cite{Jun2012} and Flickinger et al. \cite{Flickinger2015} respectively. The provided application \textbf{VerifyBamId}\footnote{\url{http://genome.sph.umich.edu/wiki/VerifyBamID}} is applying likelihood-based methods for detecting sample contamination based on sequence and MicroArray-based genotype data \cite{Jun2012}. This approach was applied on all samples from the 1000 Genome data set, and the resulting data release Phase 1 and Phase 3 were filtered according to the contamination-index provided by VerifyBamID. However this approach is not applicable to mtDNA NGS data, but designed for nuclear DNA contamination detection, also requiring additional data from Microarrays.

A recent approach was presented by Dickins at al. \cite{Dickins2014} by describing a Galaxy \cite{Goecks2010,Afgan2016} based pipeline for contamination control in mitochondrial genomes. The pipeline as well as the data are available through the website\footnote{\url{https://usegalaxy.org/u/aun1/p/controlling-for-contamination-in-resequencing}}.  The concept of the method is based on building a Neighbor-Joining (NJ) tree \cite{Saitou1987}, for the minor allele frequency (MAF). The method is based on the assumption that contamination is manifested in a sample by multiple polymorphic sites with tight MAF distribution. An advantage of this tool is the Galaxy related reproducibility of the workflow\footnote{\url{https://usegalaxy.org/u/aun1/p/controlling-for-contamination-in-resequencing}}. The provided workflow is straightforward in small to large scale data sets such as in population studies. While this approach can have advantage over a haplogroup-based detection of contamination in some situations, it is not suitable for investigations on single profiles.  This accounts in particular for forensic investigations but also for case studies in medical research. Further the source of contamination is required to be included in the data set, not necessarily always the case.

Yet another approach is presented in the tool called \textbf{Schmutzi} by Renaud et al. \cite{Renaud2015}, for detecting contamination in ancient mitochondrial DNA samples. Based on empirical and simulated data sets, the contamination estimation tool is designed for highly degraded mtDNA also affected by cytosine deamination (loosing an amine group from the molecule, being  a process typical in ancient DNA) and problems regarding contaminations with present day DNA.  However, the absence of deamination yields incorrect estimations and therefore their tool is highly recommended to be used exclusively on mtDNA harboring the mentioned properties. The underlying framework is based on a Bayesian maximum a posteriori algorithm, which yields the most likely model parameters from the provided data. 

While the presented related tools are designed for different NGS derived data, the comparison with our new method is performed on two different data sets. For comparison with the mitochondrial NGS data, we compare the data set provided by Dickins et al. \cite{Dickins2014} to check if the haplogroup-based contamination check presented subsequently can detect the expected contamination described. We further demonstrate the use of our approach in the 1000 Genomes phase 3 data set, including 2,504 whole genome sequences from 26 populations world wide. A direct comparison with the tool from Renaud et al. could not be performed.


\section{mtDNA Phylogenetic Approach for Contamination Detection}\label{haplochecker}
We developed a tool for mitochondrial sequence data analysis for estimation of contamination accessible via a Web Service\footnote{\url{http://mtdna-server.uibk.ac.at/start.html}} and as a standalone version on Github\footnote{\url{https://github.com/haansi/greenVC}}. The underlying principle is that all variants relative to the mitochondrial reference sequence rCRS \cite{Andrews1999} can be split into minor and major variant allele frequency (VAF) \cite{Li2010, Avital2012}. The concept is visualized in Figure \ref{fig-concept-cont}. 
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{images/heteroplasmy.png}
    \caption[Display of all possible pairwise sample contamination]{Display of all possible contaminations by pairwise comparison of major/minor haplotype within one analyzed sample. Three different scenarios A) mixture of 2 samples in different branches, B) mixture on same clade where 20\% are pure H1a1 reads and C) where 80\% are in H1a1, but share remaining 20\% with H1a1a1 } 
    \label{fig-concept-cont}
\end{figure}

As an example in Figure \ref{fig-concept-cont} a contamination level of 20\% over all reads is displayed. A) Shared polymorphisms of two haplotypes are presented in a common branch (H2 to H1a1 in blue), whereas the split into diverging branches evidences the two different lineage components (see A)). A mixture of two haplotypes within a single lineage but of different lineage depths (here minor H1a1 and major H1a1a1) is evidenced if no minor component can be found  (see B)). A mixture of two haplotypes within a single lineage but of different lineage depths (here minor H1a1a1 and major H1a1) is evident if the minor components at equal levels lead to a meaningful haplogroup affiliation  (see C)). The homoplasmic sites (in blue) confirm the identification of the branching clades.

\subsection{Variant Calling}
While the method presented in Chapter \ref{chap:NGS} directly implements the approach presented in Section \ref{cont:haplochecker}, it was not designed for the purpose of detecting low-level contaminations in low-coverage data, but rather detect reliably the presence of homo- and heteroplasmic variants. To detect contamination below the 10\% level in sequence data with a coverage $<$ 100 fold, a naive variant caller was designed, based on the HTSJDK framework\footnote{\url{https://samtools.github.io/htsjdk/}}, and applies a Clopper Pearson binomial confidence interval \cite{CLOPPER1934}, representing an exact confidence interval. The number of minor alleles (x), the coverage (n) and the confidence level ($\alpha$) are provided, such that the lower limit ($ll$) and upper limit ($ul$) are provided:
\begin{equation}
 ll = \frac{1}{(1 + \frac{(n - x + 1)}{(x qf(\frac{\alpha}{2}, 2 x, 2 (n-x+1)))})}
\end{equation}
\begin{equation}
 ul = \frac{1}{(1 + \frac{(n - x)    }{ ((x + 1)  qf(1-\frac{\alpha}{2}, 2 (x+1), 2 (n-x)))})}
\end{equation}
For $x=0$ yields $ll=0$ and $ul = 1 - (\frac{\alpha}{2})^{(\frac{1}{n})}$ and for $x = n$ $ll = \frac{\alpha}{2}^{(\frac{1}{n})}$, and $ul =1$. Since those two exceptions represent homoplasmic variants (either all bases according the reference or all bases as complete exchange), the calculation can be omitted.
The quantile function $qf$ is accepting the quantile parameters for the F-distribution ( $ll_{\alpha} = \frac{\alpha}{2}$ and $ul_{\alpha}= 1-\frac{\alpha}{2}$ respectively) together with the degrees of freedom ($ll_{df_1} = 2x$, $ll_{df_2} = 2 (n-x+1)$,  $ul_{df_1} =  2 (x+1)$, $ul_{df_1} =  2 (n-x)$ ) \footnote{\url{https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Fdist.html}}. A heteroplasmy is marked as reliable, when the lower limit exceeds the user defined threshold $h_T$ ($ll  \geq h_T$). The Clopper-Pearson implementation from the Apache Commons Mathematics Java Library\footnote{\url{http://commons.apache.org/proper/commons-math}} is applied.

This rather simple variant caller provides a variety of options, to get from the BAM file to the final variants:
\begin{enumerate}
\item \textbf{bam2var}: a user defined quality score filtering is applied, and all reads with read length $>$ 20 bases are considered. Reads are handled in such manner, that the circular structure of the mtDNA is considered, and positions exceeding the length of the user-provided reference restart at base 1. 
The parameters are the BAM input file (in), the result folder (out), where the pileup file as well as the variant file and HaploGrep input files are generated, the reference sequence (ref), the variant allele frequency (VAF), applied to distinguish between heterop, and homoplasmic variants and the base quality (QUAL). The listing shows an example, VAF corresponding to 20\% here:
\begin{lstlisting}[language=bash]
java -jar greenVC.jar bam2var --in HG01500.IBS.exome.MT.bam --out resultfolder  --ref data/rcrs.fasta  --VAF 0.2 --QUAL 20
\end{lstlisting}
\item \textbf{haplocheck}: generation of the HaploGrep input files for contamination detection. The method is presented in \ref{cont:haplochecker}. Based on the previous called variants, this command generates the HaploGrep input file. Thereby major/minor allele profiles are generated to perform sample contamination detection down to the 5\% heteroplasmic level.
\begin{lstlisting}[language=bash]
java -jar greenVC.jar haplocheck --in resultfolder/variants.txt --out haplogrepinput.hsd   --VAF 0.05 
\end{lstlisting}
\item \textbf{haplocheck-mtDNA-Server}: the result in the heteroplasmies.txt file from mtDNA-Server (see Chapter \ref{chap:NGS}) is handled similar to the \textbf{haplocheck} option. Again, the input file is generated based on major/ minor allele profiles in perform the sample contamination check with HaploGrep presented in Chapter \ref{chapterHaplogrep}. Example call:
\begin{lstlisting}[language=bash]
java -jar greenVC.jar haplocheck-mtDNA-Server --in heteroplasmies.txt --out haplogrepinput.hsd  --VAF 0.05 
\end{lstlisting}
\item \textbf{lofreq}: the resulting VCF file, generated by LoFreq \cite{Wilm2012}, as an additional ultra-sensitive variant caller (able of calling variants occurring in $<$0.05\% of all reads) can further be used as data input. The HTSJDK-library\footnote{\url{https://github.com/samtools/htsjdk}} is applied to transform the VCF file into HaploGrep input profiles. LoFreq itself does not call genotypes\footnote{\url{http://csb5.github.io/lofreq/}}, so the SAMPLE and FORMAT columns in the VCF file are missing. The required information for the generation of HaploGrep files, are the VCF filename, the position, the reference and the alternative base, as well as the allele frequency, provided in the INFO
\begin{lstlisting}[language=bash]
java -jar greenVC-0.1.jar lofreq --in inputfile.vcf --out haplogrepinput.hsd 
\end{lstlisting}
\item \textbf{GUI}: the application has a simple Swing GUI, shown if no parameter is provided. The bam2var step is calculated based on the parameters previously described.
\end{enumerate}
The presented method requires manual inspection of the HaploGrep results, while this is not explicitly the case for the pipeline presented thereafter. 
\subsection{HaploChecker}\label{cont:haplochecker}
Haplochecker starts with the called pileup variant file, with the variants in tab-spaced file format. Shared homoplasmic mutations help identifying the source of contamination, indicating mutual variants along phylogenetic branches. The contamination detection approach is also implemented directly in mtDNA-Server pipeline as drafted in Chapter \ref{chap:NGS} for FASTQ or BAM files. On the user interface which is automatically generated by Cloudgene, the files for upload are expected to include the following indications: Sample ID (ID), mtDNA Position (POS), Major Allele (WTALLELE), Variant Allele (MUTALLELE) and the minor variant allele frequency (VAF). The threshold for heteroplasmy level detection is user defined and is employed to differentiate between heteroplasmic and homoplasmic variants. After the file upload is performed, HaploGrep  profiles are generated directly in the pipeline by separating the minor and major component of a heteroplasmic variant accordingly to the resulting profiles. HaploGrep is based on the constantly updated worldwide mtDNA phylogeny Phylotree of currently 5,437 haplogroups\footnote{\url{Phylotree version 17, (18 Feb 2016)}}. The generated profiles are subsequently checked for haplogroup (hg) consistency. The generated report indicates possible contaminations based on phylogenetic differences. Figure \ref{cont:workflow} represents the overview of this workflow. It is possible to provide both either heteroplasmic and homoplasmic sites or heteroplasmic sites only. The differing haplogroup affiliations of major and minor component are reported per sample along with a score value indicating the following additional information: 
\begin{enumerate}[label=(\alph*)]
\item heteroplasmies found in the resulting haplogroup, 
\item positions not covered under this haplogroup affiliation, 
\item heteroplasmies not showing any impact on the current phylogeny. Those can be either according the rCRS or never be observed in Phylotree 17, called as \textit{global private mutations} or heteroplasmic variants found in other haplogroups (called \textit{local private mutations}).
\end{enumerate}
For all polymorphisms found in the resulting haplogroup, the standard deviation and the mean value of the VAF of heteroplasmic mutations are calculated, and the results are represented as box-plots. 
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{images/workflow_haplochecker.png}
    \caption[Cloudgene pipeline for contamination detection]{Cloudgene based pipeline for contamination detection. Based on a text file and the provided heteroplasmic level to distinguish (default 1\%), the pipeline generates the HaploGrep input file (1) with major and minor components, estimates the haplogroups  of both components (2) and lists contamination issues in an HTML-Report (3).} 
    \label{cont:workflow}
\end{figure}

The pipeline was defined as a YAML file for the generation of the Cloudgene based workflow, abstracted in the Figure \ref{cont:workflow}. The workflow joins two JAR files, one for the generation of the HaploGrep profiles (Figure \ref{cont:workflow} (1)) and the second representing a console version of HaploGrep (Figure \ref{cont:workflow} (2)). The logic for contamination detection is directly implemented in R (Figure \ref{cont:workflow} (3)). The R script accepts the HaploGrep result files, and the input files for the dynamic HTML report, generated with R Markdown. 

\section{Validation}
The evaluation of the herein developed Haplochecker system was performed using simulated data. Here we randomly mixed haplotypes of known haplogroups in silico and generated test profiles. As contamination detection within haplogroup H (approximately 45\% within Europe) is critical due to the comparison or largely similar sequences according to the European reference (rCRS), we placed particular emphasis on the validation of sequences belonging to haplogroup H. Therefore, we mixed all neighboring samples assigned to haplogroup H (858) and again generated test profiles. Subsequently, the major and minor profiles of the haplotypes were retrieved by assigning different variant allele frequencies for each shared, minor and major mutations. Shared mutations were denoted with VAF 1, major variants $>$0.5 and minor contributions $\leq$0.5, respectively. For these simulations, we used data of the most recent phylogeny based on Phylotree 17, comprising 4,560 variants (3,740 transitions, 399 transversions, 50 inserts, 50 deletions and 123 back mutations), defining 5,435 haplogroups.  


\section{Results}\label{cont:result}
We applied the HaploChecker pipeline on the publicly available data from the 1000 Genomes Project Phase 3 \cite{Sudmant2015,Auton2015} as well as on data presented with known sample contamination issues in Dickins et al. \cite{Dickins2014} for validation purpose. Thereby we compared our approach indirectly to the VerifyBamId, which was applied in the 1000 Genomes data. We directly compare 
HaploChecker to the Galaxy Contamination Detection pipeline\footnote{\url{https://usegalaxy.org/u/aun1/p/controlling-for-contamination-in-resequencing}}.

\subsection{1000G phase 3 data}
The download of the chromosome MT from the BAM files resulted in a data volume of 97 GB. In total 2,504 samples were analyzed, as inferred from the sample information provided on the public available sample information. In total 1,174,564,365 reads were processed on mtDNA-Server with the previously described parameters yielding to 12,489 heteroplasmic sites over all samples, corresponding 4.98 heteroplasmic variants per sample. Heteroplasmies are manifested on 5,065 sites out of all possible 16,569 sites. 
The data previously described and processed by the pipeline shows several samples with a high number of heteroplasmic variants $\geq$ 20 sites. Large amount of those samples can be identifies as contaminated (see Figure \ref{cont:1000G}). When considering the coverage distribution over the 2,504 samples, a cluster becomes apparent that has a low coverage of $\leq$ 500 fold. Investigating the reason, it becomes visible that different tissues where used for the extraction of the DNA. Therefore when making statements about the copy number derived from NGS runs, it is of extreme importance not to mix samples where different DNA extraction methods are applied.

%Since all data are generated by whole genome sequencing on the same vendor's devices, the coverage reflects the relative amount of mitochondrial copy number to the nuclear DNA. Taking the provided 1000G sample information into account, the variance in the mean coverage can be explained by the origin of the DNA extraction. Samples with DNA extracted from blood show a much lower mitochondrial copy number (mtCN) compared to samples where the DNA was extracted from the lymphoblastoid cell lines (LCL). The mitochondrial copy number is often calculated as $m$ or $mtCN$, by calculating the ratio between reads mapping to the mitochondrial genomes ($r_m$), to the reads which map to the nuclear genome ($r_n$)\cite{Reznik2016}.
%\begin{equation}
%mtCN_1 = \frac{r_m}{r_n}
%\end{equation}
%Ding et al. \cite{Ding2015} further consider the two copies of autosomal DNA in the cell, by taking the average coverage of the mtDNA ($avgCov_m$) and the autosomal DNA ($avgCov_n$) into account:
%\begin{equation}
%mtCN_2 = \frac{avgCov_m}{avgCov_n}\times{2}
%\end{equation}
%By applying these equations to samples with a mean coverage of 100 x on the mtDNA and 4 x on the nuclear DNA, the $mtCN_2=100/4*2 = 50$ instead of  $mtCN_1=100/4$ =25. It is described in the literature that samples

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{images/contamination1000g.png}
    \caption[Contaminated Samples in 1000G phase 3]{Contaminated Samples in 1000G phase 3, the x-axis represents the mean coverage per sample, and the y-axis represents the amount of heteroplasmic sites per sample. The color indicates if a sample is affected by contamination.} 
    \label{cont:1000G}
\end{figure}
\subsection{Pipeline Comparison with existing Solutions}
While the direct comparison to the method from Renaud et al. \cite{Renaud2015} is not feasible for non-degraded DNA, the comparison to the Galaxy pipeline \cite{Dickins2014} as well as the VerifyBamId \cite{Flickinger2015} is represented in this subsection. The different setups for the comparison are the following: 
\begin{enumerate}
\item Experiment 1: The data represented in \cite{Dickins2014} was downloaded resulting in 13GB of paired end FASTQ files. The processing of the data was done with a private mtDNA-Server installation, and the samples were assessed for contamination with the herein presented method. Subsequently the results from Dickins et al. were compared. Figure \ref{cont:family} summarizes the contaminated samples in the data provided. Table \ref{comptable} represents the results from the Galaxy Pipeline and the results from the contamination pipeline directly implemented in mtDNA-Server.
 \begin{minipage}{\linewidth}
    \centering
    \includegraphics[width=0.7\textwidth]{images/families.png}
    \captionof{figure}[Contamination represented in the Family mtDNA data]{Contamination represented in the Family mtDNA data  \cite{Dickins2014} analyzed with the herein described method regarding contamination. As can be seen, the different families F301 (n=5), F316 (n=5), F36 (n=6), F41 (n=5), F46 (n=4) and F54 (n=3) all show traces of contamination with other family's group members. Each circle represents a family, the color represented in the legend indicate what other mtDNA contributed the contaminations.} 
    \label{cont:family}
\end{minipage}

\begin{table}[]
\centering
\caption[Table to Experiment 1]{Table to Experiment 1: Comparison of Galaxy pipeline to the herein presented approach. Sites represents the heteroplasmic variants found in each sample, Status for the Galaxy Pipeline are classified in yes (fail), possible (warn) and no (pass). The value in HaploChecker indicates the additional haplogroup found, indicating the source of contamination. }
\label{comptable}
\begin{tabular}{|l|ll|ll|}
\hline
     & \multicolumn{2}{c|}{\textbf{Dickins et al.}} & \multicolumn{2}{c|}{\textbf{HaploChecker}} \\
\hline
\textbf{Sample}       & \textbf{Sites}     & \textbf{Status}     & \textbf{Sites}        & \textbf{Contamination}        \\
F301-M468-BL &35 & yes (fail) &30 &yes (H3af) \\
F301-M468C1-BL &40 & yes (fail) &45 &yes (H3af) \\
F301-M468C2-BL &47 & yes (fail) &47 &yes (K2a10) \\
F301-M468C3-BL &33 & yes (fail) &51 &yes (K2a10)  \\
F301-M468C4-BL &39 &possible (warn) &64 &yes (U2e2a1) \\
F316-M483-BL &38 & yes (fail) &65 &yes (J1b1a1a) \\
F316-M483C1-BL &41 &yes (fail) &46 &yes (J1b1a1a) \\
F316-M483C2-BL &25 & yes (fail) &31 &yes (J1c1a) \\
F316-M483C3-BL &8 &possible (warn) &3 & no\\
F316-M483C4-BL &6 &possible (warn) &0 & no \\
F36-M45-BL &5 &no (pass) &1 & no\\
F36-M45C1-BL &8 &possible (warn) &0 & no \\
F36-M45C2-BL &7 &possible (warn) &3 & no\\
F36-M45C3-BL &43 & yes (fail) &38 &yes (J1b1a1a) \\
F36-M45C4-BL &42 & yes (fail) &39 &yes (J1b1a1a) \\
F36-M46-BL &12 & yes (fail) &41 &yes (J1b1a1a) \\
F41-M51-BL &9 &possible (warn) &13 &yes (J1b1a) \\
F41-M51C1-BL &6 &possible (warn) &2 & no\\
F41-M51C2-BL &7 &possible (warn) &2 & no\\
F41-M52-BL &54 &yes (fail) &59 &yes (U2e2a1) \\
F41-M52C1-BL &39 & yes (fail) &52 &yes (U2e2a1) \\
F46-M57-BL &46 & yes (fail) &59 &yes (J1b1a1a) \\
F46-M58-BL &12 &possible (warn) &18 & no\\
F46-M58C1-BL &43 & yes (fail) &36 &yes (H3af) \\
F46-M58C2-BL &35 & yes (fail) &36 &yes (H3af) \\
F54-M67-BL &37 & yes (fail) &32 &yes (H3af) \\
F54-M67C1-BL &44 & yes (fail) &58 &yes (U2e2a1) \\
F54-M67C2-BL &42 & yes (fail) &51 &yes (U2e2a1)  \\
\hline
\end{tabular}
\end{table}

\item Experiment 2: The Phase 3 data released was verified with VerfifyBamID prior to publication by the 1000G consortium\footnote{\url{http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/working/20130514_phase3_verifybam_results/}}. For verifying the herein presented haplogroup based contamination detection approach, the mitochondrial sequences were downloaded from all samples marked with the verdicts \verb|HIGH_CHIP_MIX| (n=27), \verb|HIGH_FREE_MIX| (n=11) and \verb|POSSIBLE_SWAP| (n=7) from the FTP site\footnote{\url{ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/data/}}. The data comprised 37 CRAM\footnote{\url{http://www.ebi.ac.uk/ena/software/cram-toolkit}} files (8 samples showed two verdicts), and was retrieved with the latest Samtools 1.3.1. In a second step the CRAM files were converted to FASTQ files and remapped with BWA MEM to the rCRS reference sequence. Thereafter all steps were identical to the analysis of the 1000G Phase 3 low coverage data described previously.
The file size of the samples accumulated to $\sim$ 500 MB, varying significantly in size. The mean coverage over the mt-genome varies between \~100x (HG03982) and $\sim$8,000x (HG03799). Out of the 3 different contamination classifications (\verb|HIGH_CHIP_MIX|, \verb|HIGH_FREE_MIX| and \verb|POSSIBLE_SWAP|), the sample contamination in \verb|HIGH_CHIP_MIX| could be confirmed in 26 / 27 samples, where HG01912 showed a coverage too low to predict reliable heteroplasmic sites in the lower percentage levels with mtDNA-Server. When analyzed with our stand-alone variant caller greenVC\footnote{\url{https://github.com/haansi/greenVC}}, the contamination in HG01912 could be confirmed (mixture of haplogroups L3 and L2c). The samples with the verdict \verb|HIGH_FREE_MIX| could be confirmed in 8  out of 11 samples, where HG02524 and HG02525 showed a low mean coverage of 110x and 133x. Again by employing the stand-alone variant caller, also the two low covered samples could be confirmed as contaminated. Sample HG04301 could not be confirmed as contaminated based on the mitochondrial DNA, although it showed a high coverage of 2,100x. Samples marked as \verb|POSSIBLE_SWAP| did not show signs of haplogroup contamination.

\item Experiment 3: Additionally all data from the 1000G phase 3 was classified in two groups: contaminated, and contamination free, based on the HaploChecker verdict. Thereafter the mean scores for  \verb|HIGH_FREE_MIX| and \verb|HIGH_CHIP_MIX| for the Illumina Omni Chip and Affymetrix Microarray were taken into consideration. Samples, which were marked as contaminated by HaploChecker show higher contamination scores based on VerifyBamId, then the contamination free samples. The results are represented in Figure \ref{contScores}. The upper bound of 0.03 is the result of the filtering applied, based on which the data selection for the 1000G phase 3 data set was performed. The higher mean values of Contamination are as expected, indicating that the results from VerifyBamId, are in concordance with the two different platforms Illumina and Affymetrix.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{images/Result-VerifyBamId-check.pdf}
    \caption[VerifyBamId scores in the contaminated (Contamination) and contamination free (ContFree) Samples]{VerifyBamId scores in the contaminated (Contamination) and contamination free (ContFree) Samples. Omni indicates the scores from VerifyBamId regarding the Illumina Omni Chip, while Affy denotes the Affymetrix Chip scores for VerifyBamId. free\_contam represents the HIGH\_FREE\_MIX, while chip\_contam represents the HIGH\_CHIP\_MIX score}.  
    \label{contScores}
\end{figure}
\end{enumerate}

\section{Conclusion and Outlook}\label{cont:outlook}
There are many examples in the literature showing the negative impact of sequencing errors in mtDNA datasets in different areas of research. The former gold standard DNA sequencing procedures are now being replaced by NGS technologies. With NGS it is possible to retrieve huge amount of sequencing data from biological specimens in a cost-effective manner and short-time span. These new technologies come however not without problems \cite{Bandelt2012a, Just2015}. The method presented in this Chapter called HaploChecker is a step-forward in the analysis of mtDNA data derived from NGS. It allows detecting sequencing errors and spurious findings. The concept here is to combine the information distilled from the known worldwide phylogeny, based on 24,275 complete humnan mitochondrial DNA sequences\footnote{mtDNA tree Build 17 (18 Feb 2016)\url{http://phylotree.org/}}. %This tree is transformed in an XML file with over 5,400 entries and loaded into the memory. This database is queried with the JAVA based HaploGrep, by managing the XML-tree directly in memory, read once per instantiation. 

The limitations of the herein presented method are drafted in Dickins et al. \cite{Dickins2014}. However, the big advantage of this method is that contamination can be identified from any source, with already one sample sufficing.Therefore no use of additional MicroArray data is required. The limitations are identified subsequently:
\begin{enumerate}
\item Unavailable haplotypes (haplogroups): The approach here presented highly relies on the data present in the database. There is a clear publication bias in favor to European and Asian mtDNA sequences, while African and native American sequences are underrepresented \cite{Fendt2011}. Therefore the limitation will affect merely populations in such branches. 
\item Costly implementation: the authors in \cite{Dickins2014} highlight the relatively costly implementation of a search across a large panel of samples. However we have shown in Chapter \ref{chapterHaplogrep}, that a query against the in-memory database could be optimized, and that the increase of database size follows a linear increase of the run-time. Therefore this is no longer of limitation.
\item Increasing number of haplotypes: the authors argue, that an increase in the number of possible haplogroups and samples leads to a challenging interpretation of the output. As shown with the validation of the 1000G phase 3 dataset, our approach is very fast in detecting contamination in a set of 2,504 whole genome mtDNA sequences. Projects such as the 100,000 Genome Project\footnote{\url{https://www.genomicsengland.co.uk/the-100000-genomes-project/}} are surely a next challenge. However the presented  contamination detection approach can be applied on a per-sample basis. On the other hand, the approach of Dickins requires all samples to be in the same evaluation. This is not always feasible to confirm the contamination.
\end{enumerate}
For species, where there is no phylogenetic tree available, the approach of Dickins et al. presents a robust approach for the identification of contamination. While there do exist methods to detect contamination in NGS studies, the herein presented HaploChecker represents an additional tool for this purpose. Based on various datasets, we could show that the method reliably detects the presence of additional mtDNA profiles, which can be contained to an extent in the $\sim$ 1\% of all reads in an NGS run. 
In addition, it should be noticed, that post-processing settings are important for the analysis of high-throughput data and we suggest that NGS technologies need further calibration. Sample preparation, tissue type, PCR, library preparation and specially the post-processing of the data influences the final result, as highlighted in the previous Chapter \ref{chap:NGS}.

